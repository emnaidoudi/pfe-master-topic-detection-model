{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient\n",
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "import re\n",
    "import emoji\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "from spacy.language import Language\n",
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import datetime"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Collect data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "port = \"27017\"\n",
    "username = \"accretioadmin\"\n",
    "password = \"adminaccretio&2017\"\n",
    "authSource = \"admin\"\n",
    "\n",
    "def _connect_mongo(db):\n",
    "    if username and password:\n",
    "        conn = MongoClient(host=host,\n",
    "                            port=int(port),\n",
    "                            username=username,\n",
    "                            password=password,\n",
    "                           authSource=authSource)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "        \n",
    "    return conn[db]    \n",
    "\n",
    "\n",
    "def read_mongo(db, collection, query={}):\n",
    "    db = _connect_mongo(db=db)\n",
    "    cursor = db[collection].find(query)\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_mongo(\"connectTimeline\", \"post\")\n",
    "df[\"post_id\"] = df[\"_id\"].astype(str)\n",
    "df = df[[\"post_id\",\"registrationNumber\",\"body\"]]\n",
    "df[\"registrationNumber\"] = df[\"registrationNumber\"].astype(str)\n",
    "\n",
    "df = df.dropna()\n",
    "df = df[:300]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-4-51255b84f09e>, line 10)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-4-51255b84f09e>\"\u001b[0;36m, line \u001b[0;32m10\u001b[0m\n\u001b[0;31m    if(d.pos_ != \"PROPN\" and d.pos_ != \"NUM\" and d.pos_ != \"AUX\" and d.pos_ !='DET' and d.pos_ !='ADJ' d.pos_ !='ADV'):\u001b[0m\n\u001b[0m                                                                                                       ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "@Language.factory('french_lemmatizer')\n",
    "def create_lemmatizer(nlp, name):\n",
    "    return LefffLemmatizer()   \n",
    "\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "nlp.add_pipe('french_lemmatizer', name='lefff')  \n",
    "def replace_lemma(text):  \n",
    "    doc = nlp(text)\n",
    "    for d in doc:\n",
    "        if(d.pos_ != \"PROPN\" and d.pos_ != \"NUM\" and d.pos_ != \"AUX\" and d.pos_ !='DET' and d.pos_ !='ADJ' and d.pos_ !='ADV'):\n",
    "            text = text.replace(d.text,  d.lemma_ )\n",
    "        else:\n",
    "            text = text.replace(d.text, '')\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _remove_emoji(string):\n",
    "    return emoji.get_emoji_regexp().sub(u'', string)\n",
    "\n",
    "final_stopwords_list = list(fr_stop) + list(en_stop)\n",
    "def custom_stopwords(text):\n",
    "    #common_words    \n",
    "    text = \" \".join([w for w in text.split() if w.lower() not in final_stopwords_list])\n",
    "    #HTML TAGS\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    #links\n",
    "    text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", text)  \n",
    "    #alpha_numeric\n",
    "    text = re.sub(r'[^a-zA-Z0-9 àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔ\\'ŒÙÛÜŸÇ]+', '', text)\n",
    "    # remove emojis\n",
    "    text = _remove_emoji(text)\n",
    "    #remove duplicate space\n",
    "    clear_text = \" \".join(text.split())\n",
    "    \n",
    "    return clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare():\n",
    "    global df\n",
    "    df[\"body\"] = df[\"body\"].apply(custom_stopwords)\n",
    "    df = df[df['body'].map(len)>1]\n",
    "    df[\"body\"] = df[\"body\"].apply(replace_lemma)\n",
    "prepare()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_clouds(top_words, df):\n",
    "    current_date = datetime.date.today().isoformat()\n",
    "    df[\"Topic\"] = df[\"Topic\"].astype(str)     \n",
    "    topics = list()\n",
    "    for topic, value in top_words.items():\n",
    "        if str(topic ) != \"-1\":\n",
    "            topic_dict = dict()  \n",
    "            topic_dict[\"idtopic\"] = topic\n",
    "            topic_dict[\"assigned_name\"] = value[1][0]\n",
    "            topic_dict[\"date\"] = current_date\n",
    "            topic_dict[\"importance\"] = len(list(df[df[\"Topic\"] == str(topic)][\"posts\"])[0])\n",
    "            topic_dict[\"posts\"] =  list(df[df[\"Topic\"] == str(topic)][\"posts\"])[0]\n",
    "            words = list()\n",
    "            word_dict = dict()\n",
    "            for i in value:\n",
    "                word_dict[\"text\"] = i[0]\n",
    "                word_dict[\"weight\"] = i[1]   \n",
    "                words.append(word_dict.copy())\n",
    "            words.append({\"idtopic\":topic, \"assigned_name\": value[1][0]})    \n",
    "            topic_dict[\"word-cloud\"]  = words\n",
    "            topics.append(topic_dict.copy())\n",
    "    return topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def order_people(ch):\n",
    "    l = ch.split()\n",
    "    temp = l[:]\n",
    "    l.sort(key=lambda x:temp.count(x), reverse= True)\n",
    "    return list(dict.fromkeys(l))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=final_stopwords_list).fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, docs, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    topic_sizes = extract_topic_sizes(docs)\n",
    "    # sort the dict to get the HOT topics ordered\n",
    "    top_n_words = OrderedDict(top_n_words)\n",
    "    key_order = list(topic_sizes[\"Topic\"].values)\n",
    "    for k in key_order:\n",
    "        top_n_words.move_to_end(k)\n",
    "    top_n_words = dict(top_n_words)    \n",
    "    return word_clouds(top_n_words, docs_per_topic)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model():\n",
    "    data = df[\"body\"].values  \n",
    "    #embeddings\n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "    embeddings = model.encode(data)  \n",
    "    # umap is a  dimensionality reduction algorithm\n",
    "    umap_embeddings = umap.UMAP(n_neighbors=2, \n",
    "                            n_components=3, \n",
    "                            metric='cosine').fit_transform(embeddings)\n",
    "    #Clustering\n",
    "    cluster = hdbscan.HDBSCAN(min_cluster_size=17,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)\n",
    "    \n",
    "    docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
    "    docs_df[\"posts\"] = df[\"post_id\"].astype(\"str\").values\n",
    "    docs_df['Topic'] = cluster.labels_\n",
    "    docs_df['Doc_ID'] = range(len(docs_df))\n",
    "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join, 'posts': ' '.join})#\n",
    "    # concatenate docs with same topic (with space) ---->  'Doc': ' '.join\n",
    "    docs_per_topic[\"posts\"] =docs_per_topic[\"posts\"].apply(lambda x: order_people(x))\n",
    "    #TF-IDF\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, docs_df, n=10)  \n",
    "       \n",
    "    return top_n_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words = build_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = nlp(u\"Apple mieux vaut tard que jamais cherche a acheter une startup anglaise pour 1 milliard de dollard\")\n",
    "for d in doc:\n",
    "    print(d.text, d.pos_, d._.lefff_lemma, d.tag_, d.lemma_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pymongo as pm\n",
    "\n",
    "myclient = pm.MongoClient(host=\"localhost\",\n",
    "                    port=27017,\n",
    "                    username=\"accretioadmin\",\n",
    "                    password=\"adminaccretio&2017\",\n",
    "                   authSource=\"admin\")\n",
    "\n",
    "mydb = myclient[\"topic_detection\"]\n",
    "mycol = mydb[\"topic\"]\n",
    "mycol.insert_many(top_n_words)\n",
    "\n",
    "# for i in top_n_words:\n",
    "#     d = i\n",
    "#     mycol.update({}, {\"$push\": {\"word-clouds\":d }}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(top_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_words[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
