{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import umap\n",
    "import hdbscan\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd \n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "import pandas as pd\n",
    "from pymongo import MongoClient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "host = \"localhost\"\n",
    "port = \"27017\"\n",
    "username = \"accretioadmin\"\n",
    "password = \"adminaccretio&2017\"\n",
    "authSource = \"admin\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def _connect_mongo(db):\n",
    "    if username and password:\n",
    "        conn = MongoClient(host=host,\n",
    "                            port=int(port),\n",
    "                            username=username,\n",
    "                            password=password,\n",
    "                           authSource=authSource)\n",
    "    else:\n",
    "        conn = MongoClient(host, port)\n",
    "        \n",
    "    return conn[db]    \n",
    "\n",
    "\n",
    "def read_mongo(db, collection, query={}):\n",
    "    db = _connect_mongo(db=db)\n",
    "    cursor = db[collection].find(query)\n",
    "    df =  pd.DataFrame(list(cursor))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = read_mongo(\"connectTimeline\", \"post\")\n",
    "df = df[[\"_id\",\"registrationNumber\",\"body\"]]\n",
    "df[\"registrationNumber\"] = df[\"registrationNumber\"].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji\n",
    "import spacy\n",
    "from spacy_lefff import LefffLemmatizer, POSTagger\n",
    "from spacy.language import Language"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy.lang.fr.stop_words import STOP_WORDS as fr_stop\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as en_stop\n",
    "\n",
    "final_stopwords_list = list(fr_stop) + list(en_stop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_emoji(string):\n",
    "    return emoji.get_emoji_regexp().sub(u'', string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_stopwords(text):\n",
    "    #common_words\n",
    "    text = \" \".join([w for w in text.split() if w.lower() not in final_stopwords_list])\n",
    "    #HTML TAGS\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    #links\n",
    "    text = re.sub(\"https?:\\/\\/.*[\\r\\n]*\", \"\", text)  \n",
    "    #alpha_numeric\n",
    "    text = re.sub(r'[^a-zA-Z0-9 àâäèéêëîïôœùûüÿçÀÂÄÈÉÊËÎÏÔ\\'ŒÙÛÜŸÇ]+', '', text)\n",
    "    # remove emojis\n",
    "    text = remove_emoji(text)\n",
    "    #remove duplicate space\n",
    "    clear_text = \" \".join(text.split())\n",
    "    \n",
    "    return clear_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"body\"] = df[\"body\"].apply(custom_stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df[df['body'].map(len)>1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### POS and Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@Language.factory('french_lemmatizer')\n",
    "def create_lemmatizer(nlp, name):\n",
    "    return LefffLemmatizer()\n",
    "\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "nlp.add_pipe('french_lemmatizer', name='lefff')     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def replace_lemma(text):    \n",
    "    doc = nlp(text)\n",
    "    for d in doc:\n",
    "        if(d.pos_ != \"PROPN\" and d.pos_ != \"NUM\" and d.pos_ != \"AUX\" and d.pos_ !='DET'):\n",
    "            text = text.replace(d.text,  d.lemma_ )\n",
    "        else:\n",
    "            text = text.replace(d.text, '')\n",
    "    return text   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"body\"] = df[\"body\"].apply(replace_lemma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # don't show in rapport\n",
    "# df[\"body\"] = df[\"body\"].apply(lambda words: ' '.join(word.lower() for word in words.split() if word.lower() not in final_stopwords_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # don't show in rapport\n",
    "# df[\"body\"] = df[\"body\"].apply(lambda text :re.sub(' +', ' ', text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding :  a clustering of word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = df.body.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "embeddings = model.encode(data, show_progress_bar=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[7] )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "embeddings[:3]  # a vector for each post (document)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(embeddings[1]) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dimensionality reduction : umap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# umap is a  dimensionality reduction algorithm\n",
    "umap_embeddings = umap.UMAP(n_neighbors=2, \n",
    "                            n_components=3, \n",
    "                            metric='cosine').fit_transform(embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(umap_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(umap_embeddings[1])  # n_components"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster = hdbscan.HDBSCAN(min_cluster_size=17,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(list(set(cluster.labels_)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data\n",
    "umap_data = umap.UMAP(n_neighbors=2, n_components=2, min_dist=0.0, metric='cosine').fit_transform(embeddings)\n",
    "result = pd.DataFrame(umap_data, columns=['x', 'y'])\n",
    "result['labels'] = cluster.labels_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize clusters\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "outliers = result.loc[result.labels == -1, :]\n",
    "clustered = result.loc[result.labels != -1, :]\n",
    "plt.scatter(outliers.x, outliers.y, color='#BDBDBD')\n",
    "plt.scatter(clustered.x, clustered.y, c=clustered.labels,  cmap='hsv_r')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topic per document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def order_people(ch):\n",
    "#     l = ch.split()\n",
    "#     temp = l[:]\n",
    "#     l.sort(key=lambda x:temp.count(x), reverse= True)\n",
    "#     return list(dict.fromkeys(l))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a single document for each cluster of documents:\n",
    "#all documents in a single cluster are considered as one documments\n",
    "\n",
    "docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
    "docs_df[\"posts\"] = df[\"_d\".values\n",
    "docs_df['Topic'] = cluster.labels_\n",
    "docs_df['Doc_ID'] = range(len(docs_df))\n",
    "docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join, 'posts': ' '.join})#\n",
    "# concatenate docs with same topic (with space) ---->  'Doc': ' '.join\n",
    "#docs_per_topic[\"posts\"] =docs_per_topic[\"posts\"].apply(lambda x: order_people(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(docs_per_topic[docs_per_topic[\"Topic\"] == 0][\"posts\"])[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs_per_topic.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF ==> to extract relevant words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words = final_stopwords_list).fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, docs, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    topic_sizes = extract_topic_sizes(docs)\n",
    "    # sort the dict to get the HOT topics ordered\n",
    "    top_n_words = OrderedDict(top_n_words)\n",
    "    key_order = list(topic_sizes[\"Topic\"].values)    \n",
    "    for k in key_order:\n",
    "        top_n_words.move_to_end(k)\n",
    "    top_n_words = dict(top_n_words)    \n",
    "    return top_n_words\n",
    "    \n",
    "\n",
    "top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, docs_df, n=10)\n",
    "#topic_sizes = extract_topic_sizes(docs_df); topic_sizes.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_clouds(top_words):\n",
    "    my_dict = dict()\n",
    "    word_cloud = list()\n",
    "    for topic, value in top_words.items():\n",
    "        words = list()\n",
    "        for i in value:\n",
    "            my_dict[\"text\"] = i[0]\n",
    "            my_dict[\"weight\"] = i[1]                       \n",
    "            words.append(my_dict.copy())\n",
    "        posts = list(docs_per_topic[docs_per_topic[\"Topic\"] == topic][\"posts\"])[0]    \n",
    "        words.append({\"posts\":posts})    \n",
    "        print(words)  \n",
    "        word_cloud.append(words)\n",
    "    return word_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud = word_clouds(top_n_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_cloud[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df[\"body\"] = df[\"body\"].apply(lambda words: ' '.join(word.lower() for word in words.split() if word.lower() not in final_stopwords_list))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
