{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import umap\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "from collections import OrderedDict\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_clouds(top_words):\n",
    "    my_dict = dict()\n",
    "    word_cloud = list()\n",
    "    for key, value in top_words.items():\n",
    "        words = list()\n",
    "        for i in value:\n",
    "            my_dict[\"text\"] = i[0]\n",
    "            my_dict[\"weight\"] = i[1]\n",
    "            words.append(my_dict.copy())       \n",
    "        word_cloud.append(words)\n",
    "    return word_cloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def c_tf_idf(documents, m, ngram_range=(1, 1)):\n",
    "    count = CountVectorizer(ngram_range=ngram_range, stop_words=\"english\").fit(documents)\n",
    "    t = count.transform(documents).toarray()\n",
    "    w = t.sum(axis=1)\n",
    "    tf = np.divide(t.T, w)\n",
    "    sum_t = t.sum(axis=0)\n",
    "    idf = np.log(np.divide(m, sum_t)).reshape(-1, 1)\n",
    "    tf_idf = np.multiply(tf, idf)\n",
    "\n",
    "    return tf_idf, count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_topic_sizes(df):\n",
    "    topic_sizes = (df.groupby(['Topic'])\n",
    "                     .Doc\n",
    "                     .count()\n",
    "                     .reset_index()\n",
    "                     .rename({\"Topic\": \"Topic\", \"Doc\": \"Size\"}, axis='columns')\n",
    "                     .sort_values(\"Size\", ascending=False))\n",
    "    return topic_sizes\n",
    "\n",
    "def extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, docs, n=20):\n",
    "    words = count.get_feature_names()\n",
    "    labels = list(docs_per_topic.Topic)\n",
    "    tf_idf_transposed = tf_idf.T\n",
    "    indices = tf_idf_transposed.argsort()[:, -n:]\n",
    "    top_n_words = {label: [(words[j], tf_idf_transposed[i][j]) for j in indices[i]][::-1] for i, label in enumerate(labels)}\n",
    "    topic_sizes = extract_topic_sizes(docs)\n",
    "    # sort the dict to get the HOT topics ordered\n",
    "    top_n_words = OrderedDict(top_n_words)\n",
    "    key_order = list(topic_sizes[\"Topic\"].values)\n",
    "    for k in key_order:\n",
    "        top_n_words.move_to_end(k)\n",
    "    top_n_words = dict(top_n_words)    \n",
    "    return word_clouds(top_n_words)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def insert_space(title):\n",
    "    title = ' '+title\n",
    "    title += ' '\n",
    "    return title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(csv_file):\n",
    "    df = pd.read_csv(csv_file, delimiter=\";\")\n",
    "    df['title']=df['title'].apply(lambda x: insert_space(x))\n",
    "    data = df[\"body\"].values  + df[\"title\"].values\n",
    "    \n",
    "    from sentence_transformers import SentenceTransformer\n",
    "\n",
    "    model = SentenceTransformer('distilbert-base-nli-mean-tokens')\n",
    "    embeddings = model.encode(data, show_progress_bar=True)  \n",
    "    # umap is a  dimensionality reduction algorithm\n",
    "    umap_embeddings = umap.UMAP(n_neighbors=2, \n",
    "                            n_components=3, \n",
    "                            metric='cosine').fit_transform(embeddings)\n",
    "    \n",
    "    cluster = hdbscan.HDBSCAN(min_cluster_size=2,\n",
    "                          metric='euclidean',                      \n",
    "                          cluster_selection_method='eom').fit(umap_embeddings)\n",
    "    \n",
    "    docs_df = pd.DataFrame(data, columns=[\"Doc\"])\n",
    "    docs_df['Topic'] = cluster.labels_\n",
    "    docs_df['Doc_ID'] = range(len(docs_df))\n",
    "    docs_per_topic = docs_df.groupby(['Topic'], as_index = False).agg({'Doc': ' '.join})\n",
    "    tf_idf, count = c_tf_idf(docs_per_topic.Doc.values, m=len(data))\n",
    "    top_n_words = extract_top_n_words_per_topic(tf_idf, count, docs_per_topic, docs_df, n=10)  \n",
    "    topic_sizes = extract_topic_sizes(docs_df)\n",
    "    \n",
    "    return top_n_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emna/.local/lib/python3.7/site-packages/torch/cuda/__init__.py:52: UserWarning: CUDA initialization: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx (Triggered internally at  /pytorch/c10/cuda/CUDAFunctions.cpp:100.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef89ae5237784ff8bd90e68227b2b9fd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, description='Batches', max=2.0, style=ProgressStyle(description_width=â€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "top_n_words = build_model(\"posts.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "current_date = datetime.date.today().isoformat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_date = {\"current_date\" : current_date}\n",
    "top_n_words.append(current_date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/emna/anaconda3/lib/python3.7/site-packages/ipykernel_launcher.py:14: DeprecationWarning: update is deprecated. Use replace_one, update_one or update_many instead.\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "import pymongo as pm\n",
    "\n",
    "myclient = pm.MongoClient(host=\"localhost\",\n",
    "                    port=27017,\n",
    "                    username=\"accretioadmin\",\n",
    "                    password=\"adminaccretio&2017\",\n",
    "                   authSource=\"admin\")\n",
    "\n",
    "mydb = myclient[\"topic_detection\"]\n",
    "mycol = mydb[\"word_clouds\"]\n",
    "\n",
    "for i in top_n_words:\n",
    "    d = i\n",
    "    mycol.update({}, {\"$push\": {\"word-clouds\":d }}, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
